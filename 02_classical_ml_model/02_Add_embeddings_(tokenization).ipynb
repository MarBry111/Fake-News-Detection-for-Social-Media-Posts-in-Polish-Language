{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82ca7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "import morfeusz2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12590b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978a110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_core = spacy.load(\"pl_core_news_lg\") # nlp\n",
    "nlp_pl = spacy.load('pl_spacy_model') # nlp37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d9c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('dkleczek/bert-base-polish-uncased-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53c32f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1adf5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd4314f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assestment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment_all</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>uniq_lemm</th>\n",
       "      <th>err</th>\n",
       "      <th>net</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nieweryfikowalne</td>\n",
       "      <td>Generalnie, jak pokazują dane i szacunki, to n...</td>\n",
       "      <td>Generalnie, jak pokazują dane i szacunki, to ...</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.083364</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prawda</td>\n",
       "      <td>Według ich (ukraińskich – przyp. Demagog) dany...</td>\n",
       "      <td>Według ich ukraińskich danych w Polsce, czy n...</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fałsz</td>\n",
       "      <td>Po pierwsze, jest 51,25 proc. ludzi tylko zasz...</td>\n",
       "      <td>Po pierwsze, jest 51,25 proc. ludzi tylko zasz...</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.045055</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prawda</td>\n",
       "      <td>Po pierwsze, system bankowy w Polsce, no, ma s...</td>\n",
       "      <td>Po pierwsze, system bankowy w Polsce, no, ma s...</td>\n",
       "      <td>0.026141</td>\n",
       "      <td>-0.081037</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fałsz</td>\n",
       "      <td>Magazyny gazu mamy pełne tylko w 60%, bo w lis...</td>\n",
       "      <td>Magazyny gazu mamy pełne tylko w 60%, bo w lis...</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         assestment                                               text  \\\n",
       "0  Nieweryfikowalne  Generalnie, jak pokazują dane i szacunki, to n...   \n",
       "1            Prawda  Według ich (ukraińskich – przyp. Demagog) dany...   \n",
       "2             Fałsz  Po pierwsze, jest 51,25 proc. ludzi tylko zasz...   \n",
       "3            Prawda  Po pierwsze, system bankowy w Polsce, no, ma s...   \n",
       "4             Fałsz  Magazyny gazu mamy pełne tylko w 60%, bo w lis...   \n",
       "\n",
       "                                          text_clean  sentiment_all  \\\n",
       "0  Generalnie, jak pokazują dane i szacunki, to ...      -0.008995   \n",
       "1  Według ich ukraińskich danych w Polsce, czy n...      -0.008995   \n",
       "2  Po pierwsze, jest 51,25 proc. ludzi tylko zasz...      -0.008995   \n",
       "3  Po pierwsze, system bankowy w Polsce, no, ma s...       0.026141   \n",
       "4  Magazyny gazu mamy pełne tylko w 60%, bo w lis...      -0.024012   \n",
       "\n",
       "   sentiment_avg  uniq_words  uniq_lemm  err  net       ADJ       ADV  \\\n",
       "0      -0.083364        33.0       29.0  3.0  1.0  0.212121  0.060606   \n",
       "1      -0.008995        18.0       17.0  0.0  3.0  0.055556  0.055556   \n",
       "2      -0.045055        24.0       24.0  0.0  1.0  0.125000  0.125000   \n",
       "3      -0.081037        19.0       19.0  1.0  1.0  0.157895  0.000000   \n",
       "4      -0.024012        23.0       23.0  2.0  2.0  0.086957  0.000000   \n",
       "\n",
       "       NOUN  \n",
       "0  0.212121  \n",
       "1  0.166667  \n",
       "2  0.083333  \n",
       "3  0.263158  \n",
       "4  0.173913  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem = pd.read_csv('../datasets/scrapped/demagog_features.csv', sep=';')\n",
    "df_dem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fac8a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_napis</th>\n",
       "      <th>sub_stan_zegara</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment_all</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>uniq_lemm</th>\n",
       "      <th>err</th>\n",
       "      <th>net</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rządy Tuska to również doprowadzenie do wyzysk...</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Rządy Tuska to również doprowadzenie do wyz...</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.195652</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absurdy i marnotrawstwo pokazuje najlepiej pro...</td>\n",
       "      <td>blisko_prawdy</td>\n",
       "      <td>Absurdy i marnotrawstwo pokazuje najlepiej pro...</td>\n",
       "      <td>-0.188746</td>\n",
       "      <td>-0.188746</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Przez 15 lat finansowaliście budowę korwety Ga...</td>\n",
       "      <td>blisko_prawdy</td>\n",
       "      <td>Przez 15 lat finansowaliście budowę korwety ...</td>\n",
       "      <td>-0.201911</td>\n",
       "      <td>-0.159423</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dzisiaj ponad 65 procent długu państwowego jes...</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Dzisiaj ponad 65 procent długu państwowego je...</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polska jest gotowa przyjąć każdego uchodźcę, k...</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Polska jest gotowa przyjąć każdego uchodźc...</td>\n",
       "      <td>-0.007072</td>\n",
       "      <td>-0.007072</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sub_napis sub_stan_zegara  \\\n",
       "0  Rządy Tuska to również doprowadzenie do wyzysk...           falsz   \n",
       "1  Absurdy i marnotrawstwo pokazuje najlepiej pro...   blisko_prawdy   \n",
       "2  Przez 15 lat finansowaliście budowę korwety Ga...   blisko_prawdy   \n",
       "3  Dzisiaj ponad 65 procent długu państwowego jes...           falsz   \n",
       "4  Polska jest gotowa przyjąć każdego uchodźcę, k...           falsz   \n",
       "\n",
       "                                          text_clean  sentiment_all  \\\n",
       "0  Rządy Tuska to również doprowadzenie do wyz...      -0.008995   \n",
       "1  Absurdy i marnotrawstwo pokazuje najlepiej pro...      -0.188746   \n",
       "2  Przez 15 lat finansowaliście budowę korwety ...      -0.201911   \n",
       "3  Dzisiaj ponad 65 procent długu państwowego je...      -0.008995   \n",
       "4  Polska jest gotowa przyjąć każdego uchodźc...      -0.007072   \n",
       "\n",
       "   sentiment_avg  uniq_words  uniq_lemm  err  net       ADJ       ADV  \\\n",
       "0      -0.195652        28.0       27.0  4.0  5.0  0.071429  0.071429   \n",
       "1      -0.188746        26.0       25.0  2.0  1.0  0.038462  0.038462   \n",
       "2      -0.159423        35.0       35.0  3.0  3.0  0.085714  0.028571   \n",
       "3      -0.008995        23.0       23.0  2.0  0.0  0.173913  0.043478   \n",
       "4      -0.007072        34.0       34.0  4.0  3.0  0.147059  0.058824   \n",
       "\n",
       "       NOUN  \n",
       "0  0.285714  \n",
       "1  0.423077  \n",
       "2  0.371429  \n",
       "3  0.260870  \n",
       "4  0.235294  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko = pd.read_csv('../datasets/oko.press/okopress_features.csv', sep=';')\n",
    "df_oko.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a208e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sub_napis', 'sub_stan_zegara', 'text_clean', 'sentiment_all',\n",
       "       'sentiment_avg', 'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV',\n",
       "       'NOUN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d27fd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        df_dem[['assestment', 'text_clean', 'sentiment_all', 'sentiment_avg', 'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV','NOUN']],\n",
    "        (df_oko[['sub_stan_zegara', 'text_clean', 'sentiment_all', 'sentiment_avg', 'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV','NOUN']]\n",
    "         .rename(columns={'sub_stan_zegara':'assestment'}))\n",
    "    ],\n",
    "    ignore_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539a34e",
   "metadata": {},
   "source": [
    "## Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "096389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = nlp_core.Defaults.stop_words\n",
    "stopwords = nlp_pl.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cdc917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    txt = txt.replace('\\n', ' ')\n",
    "    # doc = nlp_core(txt)\n",
    "    doc = nlp_pl(txt)\n",
    "    \n",
    "    words = [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc \n",
    "        if \n",
    "            not token.is_stop \n",
    "            and not token.is_punct \n",
    "            and not token.is_stop \n",
    "            and token.text != ' '\n",
    "            and token.lemma_ not in stopwords\n",
    "            and len(token.text) > 1 ]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9545b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7760/7760 [08:44<00:00, 14.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['text_clean'].progress_apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "688c6ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Czyli tak:'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean'].values[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11c9d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 7760/7760 [00:00<00:00, 293070.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tokens_str'] = df['tokens'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "877454cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['assestment', 'text_clean', 'sentiment_all', 'sentiment_avg',\n",
       "       'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV', 'NOUN', 'tokens',\n",
       "       'tokens_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef3c7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    ['assestment', 'text_clean', \n",
    "     'sentiment_all', 'sentiment_avg', 'uniq_words', 'uniq_lemm', 'err', 'net', \n",
    "     'ADJ', 'ADV', 'NOUN', 'tokens_str']\n",
    "].to_csv('../datasets/ready2use/fake_news_features_tokens.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e56edb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].str.join('|').str.get_dummies().max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03cbf3",
   "metadata": {},
   "source": [
    "## Create embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model.encode(df['text_clean'].values, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "#                             n_components=5, \n",
    "#                             metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['e0', 'e1', 'e2', 'e3', 'e4']] = 0\n",
    "\n",
    "# df[['e0', 'e1', 'e2', 'e3', 'e4']] = umap_embeddings\n",
    "\n",
    "# df_learn = df[\n",
    "#     ['assestment', 'text_clean', \n",
    "#      'sentiment_all', 'sentiment_avg',\n",
    "#      'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV', 'NOUN',\n",
    "#      'e0', 'e1', 'e2', 'e3', 'e4']\n",
    "# ]\n",
    "\n",
    "# df_learn.to_csv('../datasets/scrapped/demagog_features_emb_umap.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "#                           metric='euclidean',                      \n",
    "#                           cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f28e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data\n",
    "# umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "# result['labels'] = cluster.labels_\n",
    "\n",
    "# # Visualize clusters\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# outliers = result.loc[result.labels == -1, :]\n",
    "# clustered = result.loc[result.labels != -1, :]\n",
    "# plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=5)\n",
    "# plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=5, cmap='hsv_r')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_df = pd.DataFrame(df['text_clean'].values, columns=[\"Doc\"])\n",
    "# docs_df['Topic'] = cluster.labels_\n",
    "# docs_df['Doc_ID'] = range(len(docs_df))\n",
    "# docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=stopwords).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(df['text_clean'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "# top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "# topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686bc26",
   "metadata": {},
   "source": [
    "## Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [str(i)]) for i, doc in enumerate(df['tokens'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    documents, df['assestment'].values, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ef1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ce98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957df49",
   "metadata": {},
   "source": [
    "## Distributed Bag of Words (DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c98abf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 3276/3276 [00:00<00:00, 1327331.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample = 0, workers=-1)\n",
    "model_dbow.build_vocab([x for x in tqdm(X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f39b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3276, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.dv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "316c8ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 30/30 [00:00<00:00, 167.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(30)):\n",
    "    model_dbow.train(utils.shuffle([x for x in X_train]), total_examples=len(X_train), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0454f92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3276, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.dv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff584c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04901308,  0.02188243, -0.06371115,  0.073976  ,  0.08626302],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.infer_vector(X_train[0][0], epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94418bcd",
   "metadata": {},
   "source": [
    "## Distributed Memory (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88d33073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 3276/3276 [00:00<00:00, 2455860.57it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=vec_size, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(X_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b112fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [00:33<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(30)):\n",
    "    model_dmm.train(utils.shuffle([x for x in X_train]), total_examples=len(X_train), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b50dbf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03888087, 0.11844292, 0.09204848, 0.09457888, 0.04699333],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dmm.infer_vector(X_train[0][0], epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a4a1f",
   "metadata": {},
   "source": [
    "## Model Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24dca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e3f1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train(X_train, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5393bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['eurostat', 'umieścił', 'polskę', 'drugi', 'miejsce', 'ue', 'jeśli', 'chodzić', 'przyjmowanie', 'uchodźców', 'paść', 'liczba', '400', 'tys.', 'osób', 'oczywiście', 'są', 'lud', 'ukraina'], ['2525']>\n"
     ]
    }
   ],
   "source": [
    "model = model_dbow\n",
    "\n",
    "doc_id = 36\n",
    "# inferred_vector = model_loaded.infer_vector(['酒精', '用', '啥', '稀释'])\n",
    "inferred_vector = model.infer_vector(X_train[doc_id].words)\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "print(X_train[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "004cf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dbow.wv.most_similar('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4917b1a",
   "metadata": {},
   "source": [
    "## Apply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b83f40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[ 'e'+str(i) for i in range(vec_size*2) ]] = [new_model.infer_vector(d[0]).tolist() for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "404278c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn = df[\n",
    "    ['assestment', 'text_clean', \n",
    "     'sentiment_all', 'sentiment_avg',\n",
    "     'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV', 'NOUN',\n",
    "     'e0', 'e1', 'e2', 'e3', 'e4',\n",
    "     'e5', 'e6', 'e7', 'e8', 'e9']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "885c1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn.to_csv('../datasets/scrapped/demagog_features_emb_pl.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae133baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
