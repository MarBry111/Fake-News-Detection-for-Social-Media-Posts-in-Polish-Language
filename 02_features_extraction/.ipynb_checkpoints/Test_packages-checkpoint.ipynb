{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beffabc2",
   "metadata": {},
   "source": [
    "- https://github.com/ksopyla/awesome-nlp-polish\n",
    "- Sentiment: https://pypi.org/project/sentimentpl/\n",
    "- Auto correct: https://github.com/filyp/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1652209b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Locale' from 'icu' (/home/marek/anaconda3/envs/master_ds_39/lib/python3.9/site-packages/icu/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HerbertTokenizer, RobertaModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text, Word\n",
      "File \u001b[0;32m~/anaconda3/envs/master_ds_39/lib/python3.9/site-packages/polyglot/text.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequence, TextFile, TextFiles\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Detector, Language\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_property\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolyglot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Downloader\n",
      "File \u001b[0;32m~/anaconda3/envs/master_ds_39/lib/python3.9/site-packages/polyglot/detect/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Detector, Language\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/master_ds_39/lib/python3.9/site-packages/polyglot/detect/base.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Detecting languages\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01micu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Locale\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycld2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcld2\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Locale' from 'icu' (/home/marek/anaconda3/envs/master_ds_39/lib/python3.9/site-packages/icu/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentimentpl.models import SentimentPLModel\n",
    "from autocorrect import Speller\n",
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "import polyglot\n",
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ff0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_core = spacy.load(\"pl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216caffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████| 0.99M/0.99M [00:02<00:00, 409kB/s]\n",
      "Downloading: 100%|████████████████████████████| 577k/577k [00:01<00:00, 541kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 300/300 [00:00<00:00, 78.7kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 341/341 [00:00<00:00, 142kB/s]\n",
      "Downloading: 100%|███████████████████████████| 25.0/25.0 [00:00<00:00, 10.4kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 547/547 [00:00<00:00, 207kB/s]\n",
      "Downloading: 100%|███████████████████████████| 476M/476M [01:52<00:00, 4.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentimentPLModel(from_pretrained='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "406c7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary for this language not found, downloading...\n",
      "__________________________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "spell = Speller('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe28bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLMTokenizer'. \n",
      "The class this function is called from is 'HerbertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_herbert = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "model_roberta = RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df966dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_pl = spacy.load('pl_spacy_model') # or spacy.load('pl_spacy_model_morfeusz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78d469a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nienawidzę tego robić, tym bardziej że robię to codziennie. Ale przynajmniej mam łatwą pracę w Google.\n",
      "Sentiment: -0.014399224892258644\n",
      "\n",
      "Sentiment sentence 0: -0.10590832680463791\n",
      "Sentiment sentence 1: 0.0907885879278183\n",
      "\n",
      "AVG sentiment of sentences: -0.007559869438409805\n",
      "\n",
      "Unique words: 16\n",
      "Unique lemmas: 15 \n",
      "\n",
      "Errors: 0\n",
      "Named entities: 1 \n",
      "\n",
      "ADJ: 1 0.0625\n",
      "ADV: 3 0.1875\n",
      "NOUN: 1 0.0625\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_core(\"Nienawidzę tego robić, tym bardziej że robię to codziennie. Ale przynajmniej mam łatwą pracę w Google.\")\n",
    "lemmas_list = []\n",
    "tokens_list = []\n",
    "sentiments_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "error_n = 0\n",
    "\n",
    "adj_n = 0\n",
    "adv_n = 0\n",
    "noun_n = 0\n",
    "ent_n = 0\n",
    "\n",
    "print(doc.text)\n",
    "print(f'Sentiment: {model(doc.text).item()}\\n')\n",
    "\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    s = model(sent.text).item()\n",
    "    print(f'Sentiment sentence {i}: {s}')  \n",
    "    sentiments_list.append(s)\n",
    "print()\n",
    "print(f'AVG sentiment of sentences: {np.mean(sentiments_list)}\\n')\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "        lemmas_list.append(token.lemma_)\n",
    "        tokens_list.append(token.text)\n",
    "        corrected = spell(token.text)\n",
    "        if corrected != token.text:\n",
    "            error_n += 1\n",
    "    \n",
    "    if token.pos_ == 'ADJ': \n",
    "        adj_n += 1\n",
    "    elif token.pos_ == 'ADV':\n",
    "        adv_n += 1\n",
    "    elif token.pos_ == 'NOUN':\n",
    "        noun_n += 1\n",
    "        \n",
    "for ent in doc.ents:\n",
    "    #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    ent_n += 1\n",
    "    \n",
    "tokens_list = list(set(tokens_list))\n",
    "lemmas_list = list(set(lemmas_list))\n",
    "\n",
    "print('Unique words:', len(tokens_list))\n",
    "print('Unique lemmas:', len(lemmas_list), '\\n')\n",
    "print('Errors:', error_n)\n",
    "print('Named entities:', ent_n, '\\n')\n",
    "print('ADJ:', adj_n, adj_n/len(tokens_list))\n",
    "print('ADV:', adv_n, adv_n/len(tokens_list))\n",
    "print('NOUN:', noun_n, noun_n/len(tokens_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e3519773",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer_herbert.encode(doc.text, return_tensors=\"pt\")\n",
    "outputs = model_roberta(encoded_input)\n",
    "embeddings = outputs['pooler_output'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8cd4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
