{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beffabc2",
   "metadata": {},
   "source": [
    "- https://github.com/ksopyla/awesome-nlp-polish\n",
    "- Sentiment: https://pypi.org/project/sentimentpl/\n",
    "- Auto correct: https://github.com/filyp/autocorrect\n",
    "- other: https://github.com/sdadas/polish-nlp-resources\n",
    "- papers: https://homados.ipipan.waw.pl/?page_id=93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1652209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentimentpl.models import SentimentPLModel\n",
    "from autocorrect import Speller\n",
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b985b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4228f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer#for word embedding\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48ff0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_core = spacy.load(\"pl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "216caffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentPLModel(from_pretrained='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "406c7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe28bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_herbert = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "# model_roberta = RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df966dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_pl = spacy.load('pl_spacy_model') # or spacy.load('pl_spacy_model_morfeusz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45767664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_przyp(txt):\n",
    "    if txt != txt:\n",
    "        return np.nan\n",
    "    \n",
    "    txt_out = txt\n",
    "    \n",
    "    if \"przyp. Demagog\" in txt:\n",
    "        txt_out = txt_out.replace('(','').replace(')','').replace(' – przyp. Demagog','')\n",
    "    if \"(…)\" in txt:\n",
    "        txt_out =  txt_out.replace('(…)','')\n",
    "    if \"(...)\" in txt:\n",
    "        txt_out =  txt_out.replace('(...)','')\n",
    "    if \"[\" in txt:\n",
    "        txt_out = txt_out.replace('[','').replace(']','')\n",
    "        \n",
    "    txt_out = re.sub(\"@[A-Za-z0-9]+\",\"\",txt_out) #Remove @ sign\n",
    "    txt_out = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", txt_out) #Remove http links\n",
    "    \n",
    "    txt_out = unicodedata.normalize(\"NFKD\", txt_out) #cleaning html\n",
    "    \n",
    "    txt_out = txt_out.replace(';', '.')\n",
    "    \n",
    "    return txt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8db48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(txt):\n",
    "    \n",
    "    doc = nlp_core(txt)\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    lemmas_list = []\n",
    "    tokens_list = []\n",
    "    sentiments_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    error_n = 0\n",
    "\n",
    "    adj_n = 0\n",
    "    adv_n = 0\n",
    "    noun_n = 0\n",
    "    ent_n = 0\n",
    "\n",
    "    out_dict['sentiment_all'] = model(doc.text).item()\n",
    "    \n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        s = model(sent.text).item()\n",
    "        sentiments_list.append(s)\n",
    "    \n",
    "    out_dict['sentiment_avg'] = np.mean(sentiments_list)\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            lemmas_list.append(token.lemma_)\n",
    "            tokens_list.append(token.text)\n",
    "            corrected = spell(token.text)\n",
    "            if corrected != token.text:\n",
    "                error_n += 1\n",
    "\n",
    "        if token.pos_ == 'ADJ': \n",
    "            adj_n += 1\n",
    "        elif token.pos_ == 'ADV':\n",
    "            adv_n += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_n += 1\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        ent_n += 1\n",
    "\n",
    "    tokens_list = list(set(tokens_list))\n",
    "    lemmas_list = list(set(lemmas_list))\n",
    "\n",
    "    out_dict['uniq_words'] = len(tokens_list)\n",
    "    out_dict['sentiment_lemm'] =  len(lemmas_list)\n",
    "    out_dict['err'] =  error_n\n",
    "    out_dict['net'] = ent_n\n",
    "    out_dict['ADJ'] = adj_n/len(tokens_list)\n",
    "    out_dict['ADV'] = adv_n/len(tokens_list)\n",
    "    out_dict['NOUN'] = noun_n/len(tokens_list)\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32386f33",
   "metadata": {},
   "source": [
    "# Demagog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b242da03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/scrapped/demagog.csv', sep=';')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d88d460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[df['text'].str.len() > 0 ]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d9b541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x: clean_przyp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d469a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_txt = \"Nienawidzę tego robić, tym bardziej że robię to codziemie. Ale przynajmniej mam łatwą pracę w Google.\"\n",
    "\n",
    "# doc = nlp_core(test_txt)\n",
    "# lemmas_list = []\n",
    "# tokens_list = []\n",
    "# sentiments_list = []\n",
    "# embeddings_list = []\n",
    "\n",
    "# error_n = 0\n",
    "\n",
    "# adj_n = 0\n",
    "# adv_n = 0\n",
    "# noun_n = 0\n",
    "# ent_n = 0\n",
    "\n",
    "# print(doc.text)\n",
    "# print(f'Sentiment: {model(doc.text).item()}\\n')\n",
    "\n",
    "# for i, sent in enumerate(doc.sents):\n",
    "#     s = model(sent.text).item()\n",
    "#     print(f'Sentiment sentence {i}: {s}')  \n",
    "#     sentiments_list.append(s)\n",
    "# print()\n",
    "# print(f'AVG sentiment of sentences: {np.mean(sentiments_list)}\\n')\n",
    "\n",
    "\n",
    "# for token in doc:\n",
    "#     #print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "#     if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "#         lemmas_list.append(token.lemma_)\n",
    "#         tokens_list.append(token.text)\n",
    "#         corrected = spell(token.text)\n",
    "#         if corrected != token.text:\n",
    "#             error_n += 1\n",
    "    \n",
    "#     if token.pos_ == 'ADJ': \n",
    "#         adj_n += 1\n",
    "#     elif token.pos_ == 'ADV':\n",
    "#         adv_n += 1\n",
    "#     elif token.pos_ == 'NOUN':\n",
    "#         noun_n += 1\n",
    "        \n",
    "# for ent in doc.ents:\n",
    "#     #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "#     ent_n += 1\n",
    "    \n",
    "# tokens_list = list(set(tokens_list))\n",
    "# lemmas_list = list(set(lemmas_list))\n",
    "\n",
    "# print('Unique words:', len(tokens_list))\n",
    "# print('Unique lemmas:', len(lemmas_list), '\\n')\n",
    "# print('Errors:', error_n)\n",
    "# print('Named entities:', ent_n, '\\n')\n",
    "# print('ADJ:', adj_n, adj_n/len(tokens_list))\n",
    "# print('ADV:', adv_n, adv_n/len(tokens_list))\n",
    "# print('NOUN:', noun_n, noun_n/len(tokens_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad02a750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4891, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['text_clean'].str.len() > 1 ]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb273b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4891/4891 [11:57<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df['raw_dict'] = df['text_clean'].progress_apply(lambda x: extract_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e130e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.join( df['raw_dict'].apply(pd.Series).rename(columns={'sentiment_lemm' : 'uniq_lemm'}) ).drop('raw_dict', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf9dde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../datasets/scrapped/demagog_features.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3519773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = tokenizer_herbert.encode(doc.text, return_tensors=\"pt\")\n",
    "# outputs = model_roberta(encoded_input)\n",
    "# embeddings = outputs['pooler_output'].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4551537",
   "metadata": {},
   "source": [
    "# OKO.press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee632c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20081, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko_raw = pd.read_csv('../datasets/oko.press/query_result.tsv', sep='\\t')\n",
    "df_oko_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c4af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id wypowiedzi', 'Nazwa pola danych', 'Wartość pola danych',\n",
       "       'Autor Wypowiedzi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0253af6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2869, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko = df_oko_raw.pivot(index='Id wypowiedzi', columns='Nazwa pola danych', values='Wartość pola danych')\n",
    "df_oko.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c61491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Nazwa pola danych</th>\n",
       "      <th>sub_date</th>\n",
       "      <th>sub_hiperlacze</th>\n",
       "      <th>sub_napis</th>\n",
       "      <th>sub_napis_autor_wypowiedzi</th>\n",
       "      <th>sub_podpis</th>\n",
       "      <th>sub_stan_zegara</th>\n",
       "      <th>sub_title_text_after</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id wypowiedzi</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>20160511</td>\n",
       "      <td>http://www.polskieradio.pl/7/129/Artykul/16180...</td>\n",
       "      <td>Rządy Tuska to również doprowadzenie do wyzysk...</td>\n",
       "      <td>1067</td>\n",
       "      <td>„Sygnały Dnia”, Polskie Radio</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Fałsz - wycieka kilka razy mniej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>20160511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absurdy i marnotrawstwo pokazuje najlepiej pro...</td>\n",
       "      <td>1026</td>\n",
       "      <td>Sejm</td>\n",
       "      <td>blisko_prawdy</td>\n",
       "      <td>Jest moździerz, nie ma amunicji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>20160511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Przez 15 lat finansowaliście budowę korwety Ga...</td>\n",
       "      <td>1026</td>\n",
       "      <td>Sejm</td>\n",
       "      <td>blisko_prawdy</td>\n",
       "      <td>Niedokończony okręt za miliard złotych</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>20160516</td>\n",
       "      <td>http://www.tokfm.pl/Tokfm/1,145400,20083911,mo...</td>\n",
       "      <td>Dzisiaj ponad 65 procent długu państwowego jes...</td>\n",
       "      <td>1257</td>\n",
       "      <td>TOK FM</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Fałsz - pomylił się o 92 miliardy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>20160512</td>\n",
       "      <td>https://www.wprost.pl/kraj/10006923/Polska-jes...</td>\n",
       "      <td>Polska jest gotowa przyjąć każdego uchodźcę, k...</td>\n",
       "      <td>1076</td>\n",
       "      <td>Wywiad dla tygodnika „Mclean’s” za: Prezydent.pl</td>\n",
       "      <td>falsz</td>\n",
       "      <td>Fałsz - Duda bez pokrycia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Nazwa pola danych  sub_date  \\\n",
       "Id wypowiedzi                 \n",
       "1069               20160511   \n",
       "1172               20160511   \n",
       "1180               20160511   \n",
       "1261               20160516   \n",
       "1411               20160512   \n",
       "\n",
       "Nazwa pola danych                                     sub_hiperlacze  \\\n",
       "Id wypowiedzi                                                          \n",
       "1069               http://www.polskieradio.pl/7/129/Artykul/16180...   \n",
       "1172                                                             NaN   \n",
       "1180                                                             NaN   \n",
       "1261               http://www.tokfm.pl/Tokfm/1,145400,20083911,mo...   \n",
       "1411               https://www.wprost.pl/kraj/10006923/Polska-jes...   \n",
       "\n",
       "Nazwa pola danych                                          sub_napis  \\\n",
       "Id wypowiedzi                                                          \n",
       "1069               Rządy Tuska to również doprowadzenie do wyzysk...   \n",
       "1172               Absurdy i marnotrawstwo pokazuje najlepiej pro...   \n",
       "1180               Przez 15 lat finansowaliście budowę korwety Ga...   \n",
       "1261               Dzisiaj ponad 65 procent długu państwowego jes...   \n",
       "1411               Polska jest gotowa przyjąć każdego uchodźcę, k...   \n",
       "\n",
       "Nazwa pola danych sub_napis_autor_wypowiedzi  \\\n",
       "Id wypowiedzi                                  \n",
       "1069                                    1067   \n",
       "1172                                    1026   \n",
       "1180                                    1026   \n",
       "1261                                    1257   \n",
       "1411                                    1076   \n",
       "\n",
       "Nazwa pola danych                                        sub_podpis  \\\n",
       "Id wypowiedzi                                                         \n",
       "1069                                  „Sygnały Dnia”, Polskie Radio   \n",
       "1172                                                           Sejm   \n",
       "1180                                                           Sejm   \n",
       "1261                                                         TOK FM   \n",
       "1411               Wywiad dla tygodnika „Mclean’s” za: Prezydent.pl   \n",
       "\n",
       "Nazwa pola danych sub_stan_zegara                    sub_title_text_after  \n",
       "Id wypowiedzi                                                              \n",
       "1069                        falsz        Fałsz - wycieka kilka razy mniej  \n",
       "1172                blisko_prawdy         Jest moździerz, nie ma amunicji  \n",
       "1180                blisko_prawdy  Niedokończony okręt za miliard złotych  \n",
       "1261                        falsz       Fałsz - pomylił się o 92 miliardy  \n",
       "1411                        falsz               Fałsz - Duda bez pokrycia  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oko.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24a239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oko_fin = df_oko[['sub_napis', 'sub_stan_zegara']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9d2b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 2869/2869 [00:00<00:00, 131750.79it/s]\n",
      "/tmp/ipykernel_12048/3568069148.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_oko_fin['text_clean'] = df_oko_fin['sub_napis'].progress_apply(lambda x: clean_przyp(x))\n"
     ]
    }
   ],
   "source": [
    "df_oko_fin['text_clean'] = df_oko_fin['sub_napis'].progress_apply(lambda x: clean_przyp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7177d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2869/2869 [09:07<00:00,  5.24it/s]\n",
      "/tmp/ipykernel_12048/2899795223.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_oko_fin['raw_dict'] = df_oko_fin['text_clean'].progress_apply(lambda x: extract_features(x))\n"
     ]
    }
   ],
   "source": [
    "df_oko_fin['raw_dict'] = df_oko_fin['text_clean'].progress_apply(lambda x: extract_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3679a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oko_clean = df_oko_fin.join( df_oko_fin['raw_dict'].apply(pd.Series).rename(columns={'sentiment_lemm' : 'uniq_lemm'}) ).drop('raw_dict', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2318365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oko_clean.to_csv('../datasets/oko.press/okopress_features.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e343ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
