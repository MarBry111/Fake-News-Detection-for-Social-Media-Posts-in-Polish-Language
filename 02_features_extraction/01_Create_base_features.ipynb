{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beffabc2",
   "metadata": {},
   "source": [
    "- https://github.com/ksopyla/awesome-nlp-polish\n",
    "- Sentiment: https://pypi.org/project/sentimentpl/\n",
    "- Auto correct: https://github.com/filyp/autocorrect\n",
    "- other: https://github.com/sdadas/polish-nlp-resources\n",
    "- papers: https://homados.ipipan.waw.pl/?page_id=93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1652209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentimentpl.models import SentimentPLModel\n",
    "from autocorrect import Speller\n",
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b985b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4228f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer#for word embedding\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48ff0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_core = spacy.load(\"pl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "216caffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentPLModel(from_pretrained='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "406c7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe28bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_herbert = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "# model_roberta = RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df966dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_pl = spacy.load('pl_spacy_model') # or spacy.load('pl_spacy_model_morfeusz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b242da03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/scrapped/demagog.csv', sep=';')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d88d460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df[df['text'].str.len() > 0 ]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45767664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_przyp(txt):\n",
    "    if txt != txt:\n",
    "        return np.nan\n",
    "    \n",
    "    txt_out = txt\n",
    "    \n",
    "    if \"przyp. Demagog\" in txt:\n",
    "        txt_out = txt_out.replace('(','').replace(')','').replace(' – przyp. Demagog','')\n",
    "    if \"(…)\" in txt:\n",
    "        txt_out =  txt_out.replace('(…)','')\n",
    "    if \"[\" in txt:\n",
    "        txt_out = txt_out.replace('[','').replace(']','')\n",
    "        \n",
    "    txt_out = re.sub(\"@[A-Za-z0-9]+\",\"\",txt_out) #Remove @ sign\n",
    "    txt_out = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", txt_out) #Remove http links\n",
    "    \n",
    "    txt_out = unicodedata.normalize(\"NFKD\", txt_out) #cleaning html\n",
    "    \n",
    "    return txt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d9b541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x: clean_przyp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78d469a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nienawidzę tego robić, tym bardziej że robię to codziemie. Ale przynajmniej mam łatwą pracę w Google.\n",
      "Sentiment: -0.08739364892244339\n",
      "\n",
      "Sentiment sentence 0: -0.26721104979515076\n",
      "Sentiment sentence 1: 0.09078814089298248\n",
      "\n",
      "AVG sentiment of sentences: -0.08821145445108414\n",
      "\n",
      "Unique words: 16\n",
      "Unique lemmas: 14 \n",
      "\n",
      "Errors: 1\n",
      "Named entities: 1 \n",
      "\n",
      "ADJ: 1 0.0625\n",
      "ADV: 2 0.125\n",
      "NOUN: 2 0.125\n"
     ]
    }
   ],
   "source": [
    "test_txt = \"Nienawidzę tego robić, tym bardziej że robię to codziemie. Ale przynajmniej mam łatwą pracę w Google.\"\n",
    "\n",
    "doc = nlp_core(test_txt)\n",
    "lemmas_list = []\n",
    "tokens_list = []\n",
    "sentiments_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "error_n = 0\n",
    "\n",
    "adj_n = 0\n",
    "adv_n = 0\n",
    "noun_n = 0\n",
    "ent_n = 0\n",
    "\n",
    "print(doc.text)\n",
    "print(f'Sentiment: {model(doc.text).item()}\\n')\n",
    "\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    s = model(sent.text).item()\n",
    "    print(f'Sentiment sentence {i}: {s}')  \n",
    "    sentiments_list.append(s)\n",
    "print()\n",
    "print(f'AVG sentiment of sentences: {np.mean(sentiments_list)}\\n')\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "        lemmas_list.append(token.lemma_)\n",
    "        tokens_list.append(token.text)\n",
    "        corrected = spell(token.text)\n",
    "        if corrected != token.text:\n",
    "            error_n += 1\n",
    "    \n",
    "    if token.pos_ == 'ADJ': \n",
    "        adj_n += 1\n",
    "    elif token.pos_ == 'ADV':\n",
    "        adv_n += 1\n",
    "    elif token.pos_ == 'NOUN':\n",
    "        noun_n += 1\n",
    "        \n",
    "for ent in doc.ents:\n",
    "    #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    ent_n += 1\n",
    "    \n",
    "tokens_list = list(set(tokens_list))\n",
    "lemmas_list = list(set(lemmas_list))\n",
    "\n",
    "print('Unique words:', len(tokens_list))\n",
    "print('Unique lemmas:', len(lemmas_list), '\\n')\n",
    "print('Errors:', error_n)\n",
    "print('Named entities:', ent_n, '\\n')\n",
    "print('ADJ:', adj_n, adj_n/len(tokens_list))\n",
    "print('ADV:', adv_n, adv_n/len(tokens_list))\n",
    "print('NOUN:', noun_n, noun_n/len(tokens_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8db48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(txt):\n",
    "    \n",
    "    doc = nlp_core(txt)\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    lemmas_list = []\n",
    "    tokens_list = []\n",
    "    sentiments_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    error_n = 0\n",
    "\n",
    "    adj_n = 0\n",
    "    adv_n = 0\n",
    "    noun_n = 0\n",
    "    ent_n = 0\n",
    "\n",
    "    out_dict['sentiment_all'] = model(doc.text).item()\n",
    "    \n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        s = model(sent.text).item()\n",
    "        sentiments_list.append(s)\n",
    "    \n",
    "    out_dict['sentiment_avg'] = np.mean(sentiments_list)\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            lemmas_list.append(token.lemma_)\n",
    "            tokens_list.append(token.text)\n",
    "            corrected = spell(token.text)\n",
    "            if corrected != token.text:\n",
    "                error_n += 1\n",
    "\n",
    "        if token.pos_ == 'ADJ': \n",
    "            adj_n += 1\n",
    "        elif token.pos_ == 'ADV':\n",
    "            adv_n += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_n += 1\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        ent_n += 1\n",
    "\n",
    "    tokens_list = list(set(tokens_list))\n",
    "    lemmas_list = list(set(lemmas_list))\n",
    "\n",
    "    out_dict['uniq_words'] = len(tokens_list)\n",
    "    out_dict['sentiment_lemm'] =  len(lemmas_list)\n",
    "    out_dict['err'] =  error_n\n",
    "    out_dict['net'] = ent_n\n",
    "    out_dict['ADJ'] = adj_n/len(tokens_list)\n",
    "    out_dict['ADV'] = adv_n/len(tokens_list)\n",
    "    out_dict['NOUN'] = noun_n/len(tokens_list)\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad02a750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4891, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['text_clean'].str.len() > 1 ]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb273b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4891/4891 [11:57<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df['raw_dict'] = df['text_clean'].progress_apply(lambda x: extract_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e130e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.join( df['raw_dict'].apply(pd.Series).rename(columns={'sentiment_lemm' : 'uniq_lemm'}) ).drop('raw_dict', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf9dde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../datasets/scrapped/demagog_features.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3519773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = tokenizer_herbert.encode(doc.text, return_tensors=\"pt\")\n",
    "# outputs = model_roberta(encoded_input)\n",
    "# embeddings = outputs['pooler_output'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee632c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
