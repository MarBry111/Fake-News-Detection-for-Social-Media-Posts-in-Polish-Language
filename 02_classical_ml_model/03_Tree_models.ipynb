{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af259dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "import morfeusz2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05b7570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 7759/7759 [00:00<00:00, 266251.09it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/ready2use/fake_news_features_tokens.csv', sep=';')\n",
    "\n",
    "df = df[~df['tokens_str'].isna()]\n",
    "\n",
    "df['tokens'] = df['tokens_str'].progress_apply(lambda x: x.split(' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eac16afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['assestment'] != 'brak' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a432e",
   "metadata": {},
   "source": [
    "Na potrzeby Twojej pracy, wypowiedzi z oceną brak można odrzucić, zbity_zegar interpretować jako fałsz. Resztę danych w zależności od przyjętego modelu, jeśli ocena jest binarna to prawde i blisko_prawdy mozna polaczyc razem, tak samo jak raczej_falsz i falsz. Jeśli jednak ocena jest przedziałem to można pokusić się o przyjęcie wartości 1, 0.75, 0.5, 0.25, 0 zakładając ze 1 to prawda a 0 to pełny fałsz.\n",
    "sub_title_text_after to komentarz do wypowiedzi, moze pomóc rozwiać wątpliwości co do tego jak oceniane są wartości połówkowe i bliskie pełnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ee7af34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prawda              2817\n",
       "falsz               1308\n",
       "Fałsz               1130\n",
       "zbity_zegar          707\n",
       "Manipulacja          700\n",
       "raczej_falsz         288\n",
       "polprawda            254\n",
       "Nieweryfikowalne     236\n",
       "prawda               179\n",
       "blisko_prawdy         93\n",
       "Częściowy fałsz        7\n",
       "Name: assestment, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['assestment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0852e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'assestment'] = df['assestment'].replace({\n",
    "    'falsz' : 'Fałsz',\n",
    "    'zbity_zegar' : 'Fałsz',\n",
    "    'raczej_falsz' : 'Fałsz',\n",
    "    'prawda' : 'Prawda',\n",
    "    'blisko_prawdy' : 'Prawda',\n",
    "    'polprawda' : 'Manipulacja',\n",
    "    'Częściowy fałsz' : 'Manipulacja'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6ec3990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fałsz               3433\n",
       "Prawda              3089\n",
       "Manipulacja          961\n",
       "Nieweryfikowalne     236\n",
       "Name: assestment, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['assestment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fc5bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['assestment'] != 'Nieweryfikowalne' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3ab2cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "83139088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['documents'] = [TaggedDocument(doc, [str(i)]) for i, doc in enumerate(df['tokens'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf2d12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, id_test = train_test_split(\n",
    "    df.index.values.tolist(), \n",
    "    test_size=0.33, \n",
    "    stratify = df['assestment'].values,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e0fcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c685efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5013/5013 [00:00<00:00, 530411.59it/s]\n",
      "100%|███████████████████████████████████████████| 30/30 [00:00<00:00, 76.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, sample = 0, workers=-1)\n",
    "model_dbow.build_vocab([x for x in tqdm(df['documents'].values[id_train])])\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    model_dbow.train(utils.shuffle([x for x in df['documents'].values[id_train]]), total_examples=len(X_train), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e40d00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5013/5013 [00:00<00:00, 584462.71it/s]\n",
      "100%|███████████████████████████████████████████| 30/30 [00:31<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=vec_size, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(df['documents'].values[id_train])])\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    model_dmm.train(utils.shuffle([x for x in df['documents'].values[id_train]]), total_examples=len(X_train), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "854da590",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3760eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[ 'e'+str(i) for i in range(vec_size*2) ]] = [\n",
    "    new_model.infer_vector(d[0]).tolist() for d in df['documents'].values \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "356c1a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['assestment', 'text_clean', 'sentiment_all', 'sentiment_avg',\n",
       "       'uniq_words', 'uniq_lemm', 'err', 'net', 'ADJ', 'ADV', 'NOUN',\n",
       "       'tokens_str', 'tokens', 'documents', 'e0', 'e1', 'e2', 'e3', 'e4',\n",
       "       'e5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5fa8cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[\n",
    "    ['sentiment_all', 'sentiment_avg','uniq_words', 'uniq_lemm', 'err', 'net', \n",
    "     'ADJ', 'ADV', 'NOUN',\n",
    "     'e0', 'e1', 'e2', 'e3', 'e4', 'e5']\n",
    "].values[id_train]\n",
    "\n",
    "X_test = df[\n",
    "    ['sentiment_all', 'sentiment_avg','uniq_words', 'uniq_lemm', 'err', 'net', \n",
    "     'ADJ', 'ADV', 'NOUN',\n",
    "     'e0', 'e1', 'e2', 'e3', 'e4', 'e5']\n",
    "].values[id_test]\n",
    "\n",
    "y_train = df['assestment'].values[id_train]\n",
    "\n",
    "y_test = df['assestment'].values[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "205462f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5013, 15)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62b6006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2470, 15)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee16401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
